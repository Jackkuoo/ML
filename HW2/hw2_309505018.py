# -*- coding: utf-8 -*-
"""hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ePR1Wgs_b9JaaSxI1Idd5Y3aBD-grdtD
"""

from google.colab import drive
drive.mount('/content/gdrive')

import matplotlib.pyplot as plt
import math
import numpy as np
import pandas as pd
import cv2 
import os 
from random import shuffle 
from PIL import Image

df = pd.read_csv("/content/gdrive/MyDrive/Colab/data.csv")
print(df)  
x = df['X']
t = df['T']

"""# 1. Sequential Bayesian Learning

1.Plot five curves sampled from the parameter posterior distribution and N data points
"""

def phi(x, m, trans=1):
    X = []
    for i in range(m):
        X.append(sigmoid(x,i, m))
    if trans:
        return np.array(X).reshape(-1, m)
    else:
        return np.array(X)
def sigmoid(x, j, m, sigma=0.1):
    muj = (2*j)/ m
    a = (x - muj)/ sigma
    return 1/ (1 + np.exp(-a))
def predict(w, x):
    return w.dot(x.T)
def five_curves_sampled():
	M = 3
	s0_inv = (10**-6) * np.identity(M) #S0
	m0 = 0
	beta = 1
	data_len = [5,10,30,80]

	PHI =  phi(x[0], M)
	sn_inv = s0_inv + beta*PHI.T.dot(PHI)
	sn = np.linalg.inv(sn_inv)
	mn = sn.dot(beta*np.dot(PHI.T, t[0]))
	mn = mn.reshape(-1,)
	for i in range(1, x.shape[0]):    
	    if i in data_len:
	        plt.scatter(x[:i],t[:i], facecolor="none", edgecolor="b", label="training data")
	        plt.legend()
	        # sample five curve
	        w_sampled = np.random.multivariate_normal(mn, sn, size=5)
	        sortX = np.linspace(0, 2, 50)
	        pred = predict(w_sampled, phi(sortX, M, 0).T)
	        for j in range(5):
	            plt.title('data size %d' % i)
	            plt.plot(sortX, pred[j], '-r')
	            
	        plt.show()       
	    PHI = np.vstack((PHI, phi(x[i], M)))
	    mn_old = mn
	    sn_inv_old = sn_inv
	    sn_inv = sn_inv + beta*PHI.T.dot(PHI)
	    sn = np.linalg.inv(sn_inv)
	    mn = sn.dot(sn_inv_old.dot(mn)+beta*PHI.T.dot(t[:i+1]).reshape(-1,))

five_curves_sampled()

"""2.Plot the predictive distribution of target value t by showing the mean curve, the region of
variance with one standard deviation on both sides of the mean curve and N data points
"""

def predictive_dist(x, M, mN, SN, beta=1):
    phiX = phi(x, M, 0).T
    mean = phiX.dot(mN)
    covX = 1/beta + np.sum(phiX.dot(SN).dot(phiX.T), axis=1)
    std = np.sqrt(covX)
    return mean, std
def predictive_distribution():
	M = 3
	s0_inv = (10**-6) * np.identity(M)
	m0 = 0
	beta = 1
	MNs = []
	SNs = []
	data_len = [5,10,30,80]

	PHI =  phi(x[0], M)
	sn_inv = s0_inv + beta*PHI.T.dot(PHI)
	sn = np.linalg.inv(sn_inv)
	mn = sn.dot(beta*np.dot(PHI.T, t[0]))
	mn = mn.reshape(-1,)
	for i in range(1, x.shape[0]):    
	    if i in data_len:
	        sortX = np.linspace(0, 2, 50)
	        mean, std = predictive_dist(sortX, M, mn, sn)
	        plt.title('data size %d' % i)
	        plt.scatter(x[:i],t[:i], facecolor="none", edgecolor="b", label="training data")
	        plt.plot(sortX, mean, 'r', label='mean')
	        plt.fill_between(sortX.reshape(len(sortX)), mean-std, mean+std, alpha=0.5, color='orange', label='std')
	        plt.legend()
	        plt.show()       
	    PHI = np.vstack((PHI, phi(x[i], M)))
	    mn_old = mn
	    sn_inv_old = sn_inv
	    sn_inv = sn_inv + beta*PHI.T.dot(PHI)
	    sn = np.linalg.inv(sn_inv)
	    mn = sn.dot(sn_inv_old.dot(mn)+beta*PHI.T.dot(t[:i+1]).reshape(-1,))

predictive_distribution()

"""3.Plot the prior distributions by arbitrarily selecting two weights"""

def gaussian(x, mean, sd):
    y = 1 / (2 * np.pi) * 1 / np.sqrt(np.linalg.det(sd)) * np.exp( -0.5 * ((x - mean).T.dot(np.linalg.inv(sd))).dot((x - mean)))
    return y
def prior_distributions():
	M = 3
	s0_inv = (10**-6) * np.identity(M) #S0
	m0 = 0
	beta = 1
	MNs = []
	SNs = []
	data_len = [5,10,30,80]

	PHI =  phi(x[0], M)
	sn_inv = s0_inv + beta*PHI.T.dot(PHI)
	sn = np.linalg.inv(sn_inv)
	mn = sn.dot(beta*np.dot(PHI.T, t[0]))
	mn = mn.reshape(-1,)
	for i in range(1, x.shape[0]):    
		if i in data_len:
			w0, w1 = np.meshgrid(np.linspace(0, 5, 100), np.linspace(-5, 3, 100))
			w_combined = np.array([w0, w1]).transpose(1, 2, 0)
			N_density = np.empty((100, 100))
			for f in range(N_density.shape[0]):
				for g in range(N_density.shape[1]):
					# select weight
					N_density[f, g] = gaussian(w_combined[f, g], mn_old[:2], np.linalg.inv(sn_inv_old)[:2, :2])
			
			plt.title('data size %d' % i)
			plt.xlabel('x')
			plt.ylabel('y')
			plt.contourf(w0[0], w1[:, 0], N_density)
			plt.show()      
		PHI = np.vstack((PHI, phi(x[i], M)))
		mn_old = mn
		sn_inv_old = sn_inv
		sn_inv = sn_inv + beta*PHI.T.dot(PHI)
		sn = np.linalg.inv(sn_inv)
		mn = sn.dot(sn_inv_old.dot(mn)+beta*PHI.T.dot(t[:i+1]).reshape(-1,))

prior_distributions()

"""# 2 Logistic Regression"""

np.seterr(all='ignore')
image_size = 16
batch_size = 32

class0 = "/content/gdrive/MyDrive/Colab/Fashion_MNIST/0"
class1 = "/content/gdrive/MyDrive/Colab/Fashion_MNIST/1"
class2 = "/content/gdrive/MyDrive/Colab/Fashion_MNIST/2"
class3 = "/content/gdrive/MyDrive/Colab/Fashion_MNIST/3"
class4 = "/content/gdrive/MyDrive/Colab/Fashion_MNIST/4"

def split_train_test(): #random select 32
  split1 = []
  split2 = []
  split1 = np.random.choice(np.arange(64), 32, replace=False)
  #train.append(total[idx1])
  #split1.extend(idx1)
  split2 = list(set(np.arange(64)) - set(split1))
  #split2.extend(idx2)
  return split1, split2

from tqdm import tqdm
train_data_img = []
test_data_img = []
train_Y = []
test_Y = []
train_class0 = []
train_class1 = []
train_class2 = []
train_class3 = []
train_class4 = []

def load_data():
  for image1 in tqdm(os.listdir(class0)): 
      path = os.path.join(class0, image1)
      img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
      img1 = cv2.resize(img1, (image_size, image_size))
      train_class0.append(img1)
  a, b = split_train_test()
  for i in a:
    train_data_img.append(train_class0[i])
  for j in b:
    test_data_img.append(train_class0[j])
  for image2 in tqdm(os.listdir(class1)): 
      path = os.path.join(class1, image2)
      img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
      img2 = cv2.resize(img2, (image_size, image_size))
      train_class1.append(img2)
  a, b = split_train_test()
  for i in a:
    train_data_img.append(train_class1[i])
  for j in b:
    test_data_img.append(train_class1[j])

  for image3 in tqdm(os.listdir(class2)): 
      path = os.path.join(class2, image3)
      img3 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
      img3 = cv2.resize(img3, (image_size, image_size))
      train_class2.append(img3)
  a, b = split_train_test()
  for i in a:
    train_data_img.append(train_class2[i])
  for j in b:
    test_data_img.append(train_class2[j])

  for image4 in tqdm(os.listdir(class3)): 
    path = os.path.join(class3, image4)
    img4 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img4 = cv2.resize(img4, (image_size, image_size))
    train_class3.append(img4)
  a, b = split_train_test()
  for i in a:
    train_data_img.append(train_class3[i])
  for j in b:
    test_data_img.append(train_class3[j])

  for image5 in tqdm(os.listdir(class4)): 
    path = os.path.join(class4, image5)
    img5 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img5 = cv2.resize(img5, (image_size, image_size))
    train_class4.append(img5)
  a, b = split_train_test()
  for i in a:
    train_data_img.append(train_class4[i])
  for j in b:
    test_data_img.append(train_class4[j])

  total_data_img = np.concatenate((train_data_img,test_data_img),axis=0)
  return train_data_img, test_data_img, total_data_img

train_data_img, test_data_img, total_data_img = load_data()
print(len(train_data_img),len(test_data_img))

# Normalize
x_train = (train_data_img - np.mean(train_data_img, axis=0)) / (np.std(train_data_img, axis=0) + 1e-100)
x_test = (test_data_img - np.mean(test_data_img, axis=0)) / (np.std(test_data_img, axis=0) + 1e-100)
 
#one-hot label
y_train = np.zeros((160,5))
y_test = np.zeros((160,5))
for j in range(0, 5):
  for i in range(32*j, 32*(j+1)):
    y_train[i, j] = 1
    y_test[i, j] = 1

x_train = x_train.reshape(-1,x_train.shape[1]*x_train.shape[2]) #flatten
x_test = x_test.reshape(-1,x_test.shape[1]*x_test.shape[2]) #flatten

def shuffle(X, Y):
  randomize = np.arange(X.shape[0])
  np.random.shuffle(randomize)
  return (X[randomize], Y[randomize])
 
x_train, y_train = shuffle(x_train, y_train)
x_test, y_test = shuffle(x_test, y_test)
print("x train: ",x_train.shape) #(160,256)
print("x test: ",x_test.shape) #(160,256)
print("y train: ",y_train.shape) #(160,5)
print("y test: ",y_test.shape) #(160,5)

"""1.
(a)Implement batch GD, SGD, mini-batch
SGD (batch size = 32) and Newton-Raphson algorithms to construct a multiclass logistic
regression (b)Show the classication results of training and test data

1-1 Batch Gradient Descent
"""

def softmax2(x):
  e = np.exp(x - np.max(x))  # prevent overflow
  if e.ndim == 1:
    return e / np.sum(e, axis=0)
  else:  
    return e / np.array([np.sum(e, axis=1)]).T  # ndim = 2
def cross_entropy(yhat, y ,epsilon=1e-12):
  yhat = np.clip(yhat, epsilon, 1. - epsilon)
  ce = -np.sum(y*np.log(yhat+1e-9))
  return ce

def batch_gradient_descent(train_data,test_data,epoch):
	# initialize
	b = np.zeros(5)
	w = np.ones((train_data.shape[1],5)) #shape: (256,5)
	lr = 1e-3
	epoch = epoch
	epoch_list = []
	train_loss = []
	train_accuracy = []
	test_loss = []
	test_accuracy = []
	m = x_train.shape[0]
	#x_train (160,256) y_train (160,5) pca_feature (160,2)
    
	for e in range(epoch):
		# Calculate the value of error for loss function
		z = np.dot(train_data, w)+b #shape: (160,5)
		yhat = softmax2(z) #shape: (160,5)
		error1 = (1/m) * cross_entropy(yhat,y_train)
		d_y = y_train - yhat
		pred = np.where(yhat>0.5,1,0)
		train_accuracy.append((pred == y_train).all(axis=1).mean())
		lr = lr * 0.95
 
    #test data
		z = np.dot(test_data, w)+b #shape: (160,5)
		yhat = softmax2(z) #shape: (160,5)
		error2 = (1/m) * cross_entropy(yhat,y_test)
		pred = np.where(yhat>0.5,1,0)
		test_accuracy.append((pred == y_test).all(axis=1).mean())
 
		# Calculate the gradient
		w_grad = -np.dot(train_data.T,d_y)  #shape: (256,5)
		b_grad = -np.sum(d_y,axis=0) #shape: (5,)
 
    # Update parameters.
		w = w - lr * w_grad #shape: (256,5)
		b = b - lr * b_grad #shape: (5,)
        
		train_loss.append(error1)
		test_loss.append(error2)
		epoch_list.append(e)
		# Print loss
		if (e+1) % 10 == 0:
			print('epoch:{}, train_loss:{}, test_loss:{}\n'.format(e+1, error1,error2))
	
	print('train_accuracy:{}\ntest_accuracy:{}\n'.format(train_accuracy[e],test_accuracy[e]))
	plt.xlabel("epoch")
	plt.ylabel("loss")
	line1, = plt.plot(epoch_list, train_loss, color = 'red',label = 'train_loss')             
	line2, = plt.plot(epoch_list, test_loss, color = 'blue',label = 'test_loss')
	plt.legend(handles = [line1, line2], loc='upper right')
	plt.show()
	#plt.legend()
	plt.xlabel("epoch")
	plt.ylabel("accuracy")
	line1, = plt.plot(epoch_list, train_accuracy, color = 'red',label = 'train_accuracy')             
	line2, = plt.plot(epoch_list, test_accuracy, color = 'blue',label = 'test_accuracy')
	plt.legend(handles = [line1, line2], loc='lower right')
	plt.show()
 
batch_gradient_descent(x_train,x_test,100)

"""1-2 Minibatch Gradient Descent (batch size=32)"""

def minibatch_gradient_descent(train_data,test_data,epoch):
  # initialize
  batch_size = 32
  b = np.zeros(5)
  w = np.ones((train_data.shape[1],5)) #shape: (32,5)
  lr = 1e-3
  epoch = epoch
  test_best = 0
  train_best = 0
  epoch_list = []
  train_loss = []
  train_accuracy = []
  test_loss = []
  test_accuracy = []
  minibatch_x = []
  minibatch_y = []
  minibatch_test_x = []
  minibatch_test_y = []
  #x_train (160,256) y_train (160,5)
  for e in range(epoch):
    # Calculate the value of error for loss function
    index = (32*(e%5))
    minibatch_x = train_data[index:index+32] #shape: (32,256)
    minibatch_y = y_train[index:index+32] #shape: (32,5)
    #print("minibatch_x.shape: ",minibatch_x.shape,"minibatch_y.shape: ",minibatch_y.shape)
    z = np.dot(minibatch_x,w)+b #shape: (1,5)
    yhat = softmax2(z) #shape: (1,5)
    error1 = (1/32) * cross_entropy(yhat,minibatch_y)
    d_y = minibatch_y - yhat
    pred = np.where(yhat>0.5,1,0)
    train_accuracy.append((pred == minibatch_y).all(axis=1).mean())
    lr = lr * 0.9

    #test data
    minibatch_test_x = test_data[index:index+32] #shape: (32,256)
    minibatch_test_y = y_test[index:index+32] #shape: (32,5)
    z = np.dot(minibatch_test_x, w)+b #shape: (160,5)
    yhat = softmax2(z) #shape: (160,5)
    error2 = (1/32) * cross_entropy(yhat,minibatch_test_y)
    pred = np.where(yhat>0.5,1,0)
    test_accuracy.append((pred == minibatch_test_y).all(axis=1).mean())
   
    # Calculate the gradient
    w_grad = -np.dot(minibatch_x.T,d_y)  #shape: (256,5)
    b_grad = -np.sum(d_y,axis=0) #shape: (5,)
   
    # Update parameters.
    w = w - lr * w_grad #shape: (256,5)
    b = b - lr * b_grad #shape: (5,)

    train_loss.append(error1)
    test_loss.append(error2)
    epoch_list.append(e)

    if train_accuracy[e] > train_best:
      train_best = train_accuracy[e]
    if test_accuracy[e] > test_best:
      test_best = test_accuracy[e]

    # Print loss
    if (e+1) % 10 == 0:
      print('epoch:{}, train_loss:{}, test_loss:{}\n'.format(e+1, error1,error2))
      #print('train_accuracy:{}\ntest_accuracy:{}\n'.format(train_accuracy[e],test_accuracy[e]))

  print('train_accuracy:{}\ntest_accuracy:{}\n'.format(train_best,test_best))
  plt.xlabel("epoch")
  plt.ylabel("loss")
  plt.title("Loss")
  line1, = plt.plot(epoch_list, train_loss, color = 'red',label = 'train_loss')             
  line2, = plt.plot(epoch_list, test_loss, color = 'blue',label = 'test_loss')
  plt.legend(handles = [line1, line2], loc='upper right')
  plt.show()
  #plt.legend()
  plt.xlabel("epoch")
  plt.ylabel("accuracy")
  plt.title("Accuracy")
  line1, = plt.plot(epoch_list, train_accuracy, color = 'red',label = 'train_accuracy')             
  line2, = plt.plot(epoch_list, test_accuracy, color = 'blue',label = 'test_accuracy')
  plt.legend(handles = [line1, line2], loc='lower right')
  plt.show()

minibatch_gradient_descent(x_train,x_test,100)

"""1-3 Stochastic Gradient Descent"""

import random
def stochastic_gradient_descent(train_data,test_data,epoch):
	# initialize
	b = np.zeros(5)
	w = np.zeros((train_data.shape[1],5)) #shape: (256,5)
	lr = 1e-3
	epoch = epoch
	train_best = 0
	test_best = 0
	epoch_list = []
	train_loss = []
	train_accuracy = []
	test_loss = []
	test_accuracy = []
	m = train_data.shape[0] 
	for e in range(epoch):
		# Calculate the value of error for loss function
		random_index = random.randint(0,train_data.shape[0]-1) # random index from total samples
		#print("random_index: ",random_index)
		sample_x = train_data[random_index].reshape(1,-1) #shape: (1,256)
		sample_y = y_train[random_index].reshape(1,-1) #shape: (1,5)
		z = np.dot(sample_x,w)+b #shape: (1,5)
		yhat = softmax2(z) #shape: (1,5)
		error1 = cross_entropy(yhat,sample_y)
		d_y = sample_y - yhat
		#print(sample_y,yhat,d_y)
		pred = np.where(yhat>0.5,1,0)
		train_accuracy.append((pred == y_train).all(axis=1).mean())
		# print("sample_y.shape: ",sample_y.shape,"yhat: ",yhat.shape,"d_y: ",d_y.shape)
		lr = lr * 0.95  
		#test data
		sample_test_x = test_data[random_index].reshape(1,-1) #shape: (1,256)
		sample_test_y = y_test[random_index].reshape(1,-1) #shape: (1,5)
		z = np.dot(sample_test_x, w)+b #shape: (160,5)
		yhat = softmax2(z) #shape: (160,5)
		error2 = cross_entropy(yhat,sample_test_y)
		pred = np.where(yhat>0.5,1,0)
		test_accuracy.append((pred == sample_test_y).all(axis=1).mean())  
		# Calculate the gradient
		w_grad = -np.dot(sample_x.T,d_y) #shape: (256,5)
		b_grad = -np.sum(d_y,axis=0)  
		# Update parameters.
		w = w - lr * w_grad #shape: (256,5)
		b = b - lr * b_grad 
		train_loss.append(error1)
		test_loss.append(error2)
		epoch_list.append(e)
		if train_accuracy[e] > train_best:
			train_best = train_accuracy[e]
		if test_accuracy[e] > test_best:
			test_best = test_accuracy[e]
		# Print loss
		if (e+1) % 10 == 0:
			print('epoch:{}, Train_Loss:{}, Test_Loss:{}\n'.format(e+1, error1,error2))

	print('train_accuracy:{}\ntest_accuracy:{}\n'.format(train_best,test_best))	
	plt.xlabel("epoch")
	plt.ylabel("loss")
	plt.title("Loss")
	line1, = plt.plot(epoch_list, train_loss, color = 'red',label = 'train_loss')             
	line2, = plt.plot(epoch_list, test_loss, color = 'blue',label = 'test_loss')
	plt.legend(handles = [line1, line2], loc='upper right')
	plt.show()
	#plt.legend()
	plt.xlabel("epoch")
	plt.ylabel("accuracy")
	plt.title("Accuracy")
	line1, = plt.plot(epoch_list, train_accuracy, color = 'red',label = 'train_accuracy')             
	line2, = plt.plot(epoch_list, test_accuracy, color = 'blue',label = 'test_accuracy')
	plt.legend(handles = [line1, line2], loc='lower right')
	plt.show()

stochastic_gradient_descent(x_train,x_test,100)

"""1-4 Newton Raphson"""

def hessian(x,yhat):
	# x.T * np.diag(d_y*(h1-d_y)) * x
	hes = np.zeros((256,256))
	for i in range(256):
		for j in range(256):
			hes[i][j] = 1e-35

	out=[]
	a = np.multiply(yhat,(1-yhat)) #(160,5)
	a = np.diag(a) #(5,)
  #hes += np.dot(np.dot(x.T,a),x)
	for i in range(5):
		out.append(np.dot(np.dot(x.T,a[i]),x))
	return hes+out

def newton_raphson(train_data,test_data,epoch):
	# initialize
	b = np.zeros(5)
	w = np.zeros((train_data.shape[1],5)) #shape: (256,5)
	lr = 1e-3
	epoch = epoch
	epoch_list = []
	train_loss = []
	train_accuracy = []
	test_loss = []
	test_accuracy = []
	m = x_train.shape[0]
	train_best = 0
	test_best = 0
	beta = np.matrix((np.zeros(x_train.shape[1]),5)).T

	for e in range(epoch):
		z = np.dot(train_data, w)+b #shape: (160,5)
		yhat = softmax2(z) #shape: (160,5)
		error1 = (1/m) * cross_entropy(yhat,y_train)
		d_y = y_train - yhat
		pred = np.where(yhat>0.5,1,0)
		train_accuracy.append((pred == y_train).all(axis=1).mean())
		lr = lr*0.9
		w_grad = -np.dot(train_data.T,d_y)  #shape: (256,5)
		b_grad = -np.sum(d_y,axis=0) #shape: (5,)
		w = w - lr * w_grad
		
		c = hessian(train_data,yhat)
		
    #test data
		z = np.dot(test_data, w)+b #shape: (160,5)
		yhat = softmax2(z) #shape: (160,5)
		error2 = (1/m) * cross_entropy(yhat,y_test)
		pred = np.where(yhat>0.5,1,0)
		test_accuracy.append((pred == y_test).all(axis=1).mean())

		# Update parameters
		w = w - np.linalg.inv(c) * w_grad
		b = b - lr * b_grad

		train_loss.append(error1)
		test_loss.append(error2)
		epoch_list.append(e)
		if train_accuracy[e] > train_best:
			train_best = train_accuracy[e]
		if test_accuracy[e] > test_best:
			test_best = test_accuracy[e]
		# Print loss
		if (e+1) % 10 == 0:
			print('epoch:{}, Train_Loss:{}, Test_Loss:{}\n'.format(e+1, error1,error2))

	print('train_accuracy:{}\ntest_accuracy:{}\n'.format(train_best,test_best))
	plt.xlabel("epoch")
	plt.ylabel("loss")
	line1, = plt.plot(epoch_list, train_loss, color = 'red',label = 'train_loss')             
	line2, = plt.plot(epoch_list, test_loss, color = 'blue',label = 'test_loss')
	plt.legend(handles = [line1, line2], loc='upper right')
	plt.show()
	plt.xlabel("epoch")
	plt.ylabel("accuracy")
	line1, = plt.plot(epoch_list, train_accuracy, color = 'red',label = 'train_accuracy')             
	line2, = plt.plot(epoch_list, test_accuracy, color = 'blue',label = 'test_accuracy')
	plt.legend(handles = [line1, line2], loc='lower right')
	plt.show()
 
newton_raphson(x_train,x_test,100)

"""2.Use principal component analysis (PCA) to reduce the dimension of images to d = 2,5,10.

2-1.Repeat 1 by using PCA to reduce the dimension of images to d.
"""

def SVD(u, v, u_based_decision=True):
  #https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L500
  # columns of u, rows of v
  max_abs_cols = np.argmax(np.abs(u), axis=0)
  signs = np.sign(u[max_abs_cols, range(u.shape[1])])
  u *= signs
  v *= signs[:, np.newaxis]
  return u, v

def PCA(features, n, svd, mean=1, test=0):
  M = np.mean(features.T, axis=1)
  C = features - M # center column
  mean=1
  # eigendecomposition
  u, d, v = np.linalg.svd(C, full_matrices=False)
  u, v = SVD(u, v)
  Trans_comp = np.dot(features.T, u[:, :n])
  s = np.diag(d)
  Trans = u[:, :n].dot(s[:n, :n])
  return Trans, Trans_comp.T

"""BGD"""

d=[2,5,10]
for i in range(3):
  pca_train, pca_com = PCA(x_train, d[i], 1)
  pca_test, pca_com = PCA(x_test, d[i], 1)
  batch_gradient_descent(pca_train,pca_test,10)

"""minibatch GD"""

d=[2,5,10]
for i in range(3):
  pca_train, pca_com = PCA(x_train, d[i], 1)
  pca_test, pca_com = PCA(x_test, d[i], 1)
  minibatch_gradient_descent(pca_train,pca_test,40)

"""SGD"""

d=[2,5,10]
for i in range(3):
  pca_train, pca_com = PCA(x_train, d[i], 1)
  pca_test, pca_com = PCA(x_test, d[i], 1)
  stochastic_gradient_descent(pca_train,pca_test,40)

"""2-2.Plot d eigenvectors corresponding to top d eigenvalues

d=2
"""

pca_feature, pca_com = PCA(x_train, 2, 1)
for i in range(2):
  plt.imshow(pca_com[i].reshape(16, 16), cmap='gray')
  plt.xticks()
  plt.yticks()
  plt.title("d=2")
  plt.show()

"""d=5"""

pca_feature, pca_com = PCA(x_train, 5, 1)
for i in range(5):
  plt.imshow(pca_com[i].reshape(16, 16), cmap='gray')
  plt.xticks()
  plt.yticks()
  plt.title("d=5")
  plt.show()

"""d=10"""

pca_feature, pca_com = PCA(x_train, 10, 1)
for i in range(10):
  plt.imshow(pca_com[i].reshape(16, 16), cmap='gray')
  plt.xticks()
  plt.yticks()
  plt.title("d=10")
  plt.show()

"""3.What do the decision regions and data points look like on the vector space

3-1.Plot the decision regions and data points of the images on the span of top 2 eigenvectors by using PCA to reduce the dimension of images to 2.
"""

from matplotlib.colors import ListedColormap

def predict123(X):
  w,b = bgd(pca_train,y_combined,100)
  zz = np.dot(X, w)+b
  #print(np.argmax(zz, axis=1))
  return np.argmax(zz, axis=1)
def bgd(train_data,y_combined,epoch):
  # initialize
  b = np.zeros(5)
  w = np.ones((train_data.shape[1],5)) #shape: (256,5)
  lr = 1e-3
  epoch = epoch
  m = train_data.shape[0]
  for e in range(epoch):
    # Calculate the value of error for loss function
    z = np.dot(train_data, w)+b #shape: (320,5)
    yhat = softmax2(z) #shape: (160,5)
    error1 = (1/m) * cross_entropy(yhat,yclass)
    d_y = y_combined - yhat
    lr = lr * 0.95
    # Calculate the gradient
    w_grad = -np.dot(train_data.T,d_y)  #shape: (256,5)
    b_grad = -np.sum(d_y,axis=0) #shape: (5,)
    # Update parameters.
    w = w - lr * w_grad #shape: (256,5)
    b = b - lr * b_grad #shape: (5,)
  return w,b

def plot_decision_regions(X, y, test_idx=None, resolution=0.02):
  # setup marker generator and color map
  markers = ('s', 'x', 'o', '^', 'v')
  colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
  cmap = ListedColormap(colors[:len(np.unique(y))])

  # plot the decision surface
  x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
  xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),np.arange(x2_min, x2_max, resolution))
  #print(np.array([xx1.ravel(), xx2.ravel()]).T.shape)

  #Z = np.dot((np.array([xx1.ravel(), xx2.ravel()]).T),np.array(weights[0])) + weights[1]
  Z = predict123((np.array([xx1.ravel(), xx2.ravel()]).T))
  Z = Z.reshape(xx1.shape)
  #weights
  plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
  plt.xlim(xx1.min(), xx1.max())
  plt.ylim(xx2.min(), xx2.max())

  for idx, cl in enumerate(np.unique(y)):
    plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl)
  # highlight test samples
  if test_idx:
    # plot all samples
    if not versiontuple(np.__version__) >= versiontuple('1.9.0'):
      X_test, y_test = X[list(test_idx), :], y[list(test_idx)]
      warnings.warn('Please update to NumPy 1.9.0 or newer')
    else:
      X_test, y_test = X[test_idx, :], y[test_idx]

    plt.scatter(X_test[:, 0],X_test[:, 1],c='',alpha=1.0,linewidths=1,marker='o',s=55, label='test set')

X_combined = []
yclass = np.zeros((320,1))
X_combined = np.vstack((x_train, x_test))
y_combined = np.vstack((y_train, y_test))
for i in range(320):
  for j in range(5):
    if y_combined[i][j]==1:
      yclass[i]=j
#print(yclass)
#print("X_combined.shape: ",X_combined.shape)
pca_train, pca_com = PCA(X_combined, 2, 1)
#print("pca_train.shape: ",pca_train.shape)
#print("yclass.shape: ",yclass.shape)
bgd(pca_train,y_combined,100)
plot_decision_regions(pca_train,y_combined)

"""3-2.Repeat 3(a) by changing the order from M = 1 to M = 2"""

